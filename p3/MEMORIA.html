<!DOCTYPE html>
<html>
<head>
<title>MEMORIA.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<style>
@media print {
  body {
    margin: 1cm 1.5cm;
    font-size: 12pt;
    line-height: 1.4;
  }
  h1 {
    font-size: 16pt;
    margin-top: 0.5cm;
    margin-bottom: 0.3cm;
  }
  h2 {
    font-size: 13pt;
    margin-top: 0.4cm;
    margin-bottom: 0.3cm;
  }
  h3 {
    font-size: 11pt;
    margin-top: 0.3cm;
    margin-bottom: 0.2cm;
  }
  p {
    margin-top: 0.2cm;
    margin-bottom: 0.2cm;
  }
  table {
    margin-top: 0.2cm;
    margin-bottom: 0.2cm;
  }
  @page {
    size: A4;
    margin: 1cm 1.5cm;
  }
}
</style>
<h1 id="pr%C3%A1ctica-03-detecci%C3%B3n-en-tiempo-real">PRÁCTICA 03: DETECCIÓN EN TIEMPO REAL</h1>
<h2 id="rob%C3%B3tica-inteligente-y-aut%C3%B3noma-ria---curso-2025-2026">Robótica Inteligente y Autónoma (RIA) - Curso 2025-2026</h2>
<p><strong>Autores:</strong></p>
<ul>
<li>Pablo Hernández Martínez (pablo.hernandez.martinez@udc.es)</li>
<li>Iván Moure Pérez (i.moure@udc.es)</li>
</ul>
<hr>
<h2 id="1-introducci%C3%B3n-y-arquitectura-del-sistema">1. INTRODUCCIÓN Y ARQUITECTURA DEL SISTEMA</h2>
<h3 id="11-objetivo">1.1 Objetivo</h3>
<p>Esta práctica implementa un sistema de control del robot Robobo real mediante detección de poses corporales en tiempo real usando YOLOv8-pose. El sistema opera en dos fases secuenciales: primero, una fase de <strong>teleoperación gestual</strong> donde el usuario controla el robot mediante movimientos de brazos para aproximarlo hacia un objeto objetivo; posteriormente, una fase de <strong>aproximación autónoma</strong> donde la política de aprendizaje por refuerzo entrenada en la Práctica 01 toma el control para completar la tarea de forma autónoma.</p>
<h3 id="12-arquitectura-del-sistema">1.2 Arquitectura del Sistema</h3>
<p>El sistema integra múltiples componentes de visión por computador y control robótico:</p>
<table style="width:100%; border:none;">
<tr>
<td style="width:50%; vertical-align:top; border:none;">
<p><strong>Componentes principales:</strong></p>
<table>
<thead>
<tr>
<th>Componente</th>
<th>Tecnología</th>
</tr>
</thead>
<tbody>
<tr>
<td>Detección de poses</td>
<td>YOLOv8n-pose</td>
</tr>
<tr>
<td>Detección de objetos</td>
<td>YOLOv8n</td>
</tr>
<tr>
<td>Política RL</td>
<td>PPO (Práctica 01)</td>
</tr>
<tr>
<td>Streaming vídeo</td>
<td>robobo-python-video-stream</td>
</tr>
<tr>
<td>Control robot</td>
<td>Robobo.py</td>
</tr>
</tbody>
</table>
</td>
<td style="width:50%; vertical-align:top; border:none; padding-left:20px;">
<p><strong>Flujo de datos:</strong></p>
<p>La webcam del PC captura al usuario y alimenta YOLOv8-pose para clasificar sus gestos, mientras que la cámara del robot procesa el entorno mediante YOLOv8 para detectar el objeto objetivo. Los gestos clasificados se traducen en comandos de control para los motores del Robobo, y cuando se detecta el objetivo, se activa automáticamente la política de refuerzo aprendida.</p>
</td>
</tr>
</table>
<h3 id="13-sistema-de-control-gestual">1.3 Sistema de Control Gestual</h3>
<p>La detección de poses utiliza los 17 keypoints del modelo YOLOv8-pose. El sistema de control se basa en la posición relativa de las muñecas respecto a los hombros:</p>
<table style="width:100%; border:none;">
<tr>
<td style="width:40%; vertical-align:top; border:none;">
<table>
<thead>
<tr>
<th>Gesto</th>
<th>Condición</th>
<th>Acción Robot</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambos brazos arriba</td>
<td><code>wrist_y &lt; shoulder_y - 20</code> (ambos)</td>
<td>Avanzar (20, 20)</td>
</tr>
<tr>
<td>Brazo derecho arriba</td>
<td>Solo derecho elevado</td>
<td>Girar izquierda (0, 25)</td>
</tr>
<tr>
<td>Brazo izquierdo arriba</td>
<td>Solo izquierdo elevado</td>
<td>Girar derecha (25, 0)</td>
</tr>
<tr>
<td>Sin brazos arriba</td>
<td>Ninguno elevado</td>
<td>Detener motores</td>
</tr>
</tbody>
</table>
</td>
<td style="width:60%; vertical-align:top; border:none; padding-left:20px;">
<p><strong>Decisiones de diseño:</strong> El umbral de 20 píxeles entre muñeca y hombro proporciona robustez ante ruido en la detección sin requerir movimientos excesivamente exagerados. La inversión del control (brazo derecho → giro izquierda) resulta más intuitiva para el operador, ya que el robot gira hacia donde &quot;apunta&quot; el brazo elevado desde la perspectiva del usuario frente a la cámara. El sistema solo ejecuta acciones cuando detecta cambios de gesto, evitando comandos redundantes.</p>
</td>
</tr>
</table>
<hr>
<h2 id="2-modos-de-detecci%C3%B3n-y-transici%C3%B3n-a-pol%C3%ADtica-rl">2. MODOS DE DETECCIÓN Y TRANSICIÓN A POLÍTICA RL</h2>
<h3 id="21-detecci%C3%B3n-del-objeto-objetivo">2.1 Detección del Objeto Objetivo</h3>
<p>El sistema soporta dos modos de detección configurables mediante la variable <code>MODE</code>:</p>
<p><strong>Modo BLOB (simulador):</strong> Utiliza la detección nativa de blobs de color del Robobo para identificar objetos rojos. Ideal para pruebas en RoboboSim con el escenario del cilindro rojo.</p>
<p><strong>Modo YOLO (robot real):</strong> Emplea YOLOv8n para detectar objetos arbitrarios definidos por la variable <code>TARGET</code> (por defecto &quot;bottle&quot;). La cámara del robot transmite vídeo mediante streaming, que se procesa frame a frame para localizar el objeto con un umbral de confianza de 0.5.</p>
<h3 id="22-integraci%C3%B3n-con-pol%C3%ADtica-de-refuerzo">2.2 Integración con Política de Refuerzo</h3>
<table style="width:100%; border:none;">
<tr>
<td style="width:55%; vertical-align:top; border:none;">
<p><strong>Condiciones de activación:</strong> La política RL se activa automáticamente cuando: (1) el objeto objetivo es detectado por la cámara del robot, y (2) el usuario ha completado al menos 5 acciones de teleoperación. Este requisito mínimo garantiza que el usuario participe activamente en la fase de aproximación inicial antes de ceder el control.</p>
<p><strong>Adaptación de observaciones:</strong> La política PPO entrenada en Práctica 01 espera observaciones discretizadas en 6 sectores visuales. Para compatibilidad, las coordenadas del objeto detectado por YOLO se normalizan al rango [0, 100] y se mapean a sectores mediante <code>sector = target_x // 20</code>, manteniendo el sector 5 para cuando el objetivo no es visible.</p>
</td>
<td style="width:45%; vertical-align:top; border:none; padding-left:20px;">
<p><strong>Ejecución de la política:</strong></p>
<pre class="hljs"><code><div>observation = {<span class="hljs-string">"sector"</span>: np.array([sector])}
action, _ = model.predict(obs, deterministic=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Acciones: 0=Avanzar, 1=Izquierda, 2=Derecha</span>
<span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>:
    rob.moveWheelsByTime(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>)
<span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>:
    rob.moveWheelsByTime(<span class="hljs-number">10</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>)
<span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>:
    rob.moveWheelsByTime(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>)
</div></code></pre>
<p><strong>Condición de éxito:</strong> El episodio finaliza cuando el área del objeto detectado supera 70000 píxeles (modo YOLO) o el blob supera 8000 con posición centrada (modo BLOB).</p>
</td>
</tr>
</table>
<hr>
<h2 id="3-resultados-y-conclusiones">3. RESULTADOS Y CONCLUSIONES</h2>
<h3 id="31-demostraci%C3%B3n-del-sistema">3.1 Demostración del Sistema</h3>
<p>La funcionalidad del sistema se demuestra en dos vídeos complementarios. El primer vídeo (<code>2025-11-26 20-33-46.mp4</code>) consiste en una grabación de pantalla del ordenador que muestra simultáneamente las dos cámaras activas durante la ejecución: la cámara del móvil montado sobre el Robobo, que transmite la perspectiva del robot en tiempo real, y la webcam del ordenador donde el usuario realiza los gestos de control para dirigir el movimiento del robot según su criterio. Este vídeo permite observar la interfaz de detección de poses con las anotaciones de YOLOv8-pose, el feedback visual de las acciones detectadas, la ventana de la cámara del robot con la detección YOLO del objeto objetivo, y la activación automática de la política de refuerzo tras completar las acciones mínimas de teleoperación.</p>
<p>El segundo vídeo (<code>IMG_4853.MOV</code>) ofrece una grabación aérea que captura el comportamiento físico del Robobo durante la ejecución, proporcionando una perspectiva externa del robot navegando hacia el objetivo bajo el control gestual del usuario y posteriormente de forma autónoma.</p>
<h3 id="32-an%C3%A1lisis-del-sistema">3.2 Análisis del Sistema</h3>
<p>Entre las <strong>fortalezas</strong> del sistema destaca la integración fluida entre los componentes de visión por computador basados en YOLO y el control robótico, permitiendo una teleoperación intuitiva mediante gestos naturales que no requieren dispositivos adicionales. La reutilización exitosa de la política de refuerzo entrenada en la Práctica 01 demuestra la transferibilidad del aprendizaje entre contextos, mientras que la arquitectura modular con modos BLOB y YOLO intercambiables facilita tanto el desarrollo en simulador como el despliegue en el robot real. El feedback visual en tiempo real proporciona al operador información continua sobre el estado del sistema.</p>
<p>Respecto a las <strong>limitaciones</strong>, la latencia inherente al streaming de vídeo desde el móvil puede introducir retrasos perceptibles en la respuesta del robot ante los comandos. La detección de poses muestra sensibilidad a las condiciones de iluminación del entorno, y el umbral fijo de confianza de 0.5 podría requerir ajustes según las características específicas de cada escenario. Además, la política de refuerzo entrenada exclusivamente en simulador puede manifestar el fenómeno de sim-to-real gap al ejecutarse en el robot físico.</p>
<h3 id="33-conclusiones">3.3 Conclusiones</h3>
<p>Este trabajo demuestra la viabilidad de sistemas híbridos que combinan teleoperación gestual con control autónomo aprendido. La arquitectura modular permite alternar entre control humano y política de refuerzo de forma transparente, aprovechando las fortalezas de cada enfoque: la intuición humana para navegación de largo alcance y la precisión del agente entrenado para la aproximación final. La integración de YOLOv8 para detección de poses y objetos proporciona una base robusta y extensible para aplicaciones de robótica interactiva en tiempo real.</p>

</body>
</html>

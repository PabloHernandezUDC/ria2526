<!DOCTYPE html>
<html>
<head>
<title>MEMORIA.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<style>
@media print {
  body {
    font-size: 11pt;
    line-height: 1.3;
    text-align: justify;
  }
  h1 {
    font-size: 16pt;
  }
  h2 {
    font-size: 13pt;
  }
  h3 {
    font-size: 11pt;
  }
  p {
    text-align: justify;
  }
  table {
  }
  @page {
    size: A4;
  }
}
body {
  text-align: justify;
}
p {
  text-align: justify;
}
</style>
<h2 id="pr%C3%A1ctica-03-detecci%C3%B3n-en-tiempo-real---ria-20252026">Práctica 03: Detección en tiempo real - RIA 2025/2026</h2>
<ul>
<li>Autores: Pablo Hernández Martínez e Iván Moure Pérez</li>
</ul>
<h2 id="1-introducci%C3%B3n-y-arquitectura-del-sistema">1. INTRODUCCIÓN Y ARQUITECTURA DEL SISTEMA</h2>
<h3 id="11-objetivo">1.1 Objetivo</h3>
<p>Esta práctica implementa un sistema de control del robot Robobo mediante detección de poses corporales en tiempo real usando modelos YOLO. El sistema opera en dos fases secuenciales: primero, una fase de <strong>teleoperación</strong> donde el usuario controla el robot mediante gestos con los brazos para aproximarlo hacia un objetivo; posteriormente, una fase de <strong>aproximación autónoma</strong> donde la política de RL aprendida en la P.01 toma el control para completar la tarea: llegar al objetivo.</p>
<h3 id="12-arquitectura-del-sistema">1.2 Arquitectura del sistema</h3>
<p>El sistema integra múltiples componentes de visión por computador y control robótico: los modelos de detección YOLO, PPO para la política de RL, y las librerías de Robobo.</p>
<p>En cuanto al flujo de datos, la webcam del PC captura al usuario y alimenta YOLOv8-pose para clasificar sus gestos, mientras que la cámara del robot alimenta YOLOv8 para detectar el objeto objetivo. Los gestos clasificados se traducen en comandos de control para los motores del Robobo, y cuando se detecta el objetivo, se activa automáticamente la política de RL aprendida.</p>
<h3 id="13-sistema-de-control-gestual">1.3 Sistema de control gestual</h3>
<p>La detección de poses considera algunos de los 17 keypoints corporales del modelo YOLOv8-pose. El sistema de control se basa en la posición relativa de las muñecas respecto a los hombros:</p>
<table style="width:100%; border:none;">
<tr>
<td style="width:50%; vertical-align:top; border:none;">
<table>
<thead>
<tr>
<th>Gesto</th>
<th>Acción del robot</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ambos brazos arriba</td>
<td>Avanzar (20, 20)</td>
</tr>
<tr>
<td>Brazo derecho arriba</td>
<td>Girar izquierda (0, 25)</td>
</tr>
<tr>
<td>Brazo izquierdo arriba</td>
<td>Girar derecha (25, 0)</td>
</tr>
<tr>
<td>Sin brazos arriba</td>
<td>Detener motores</td>
</tr>
</tbody>
</table>
</td>
<td style="width:50%; vertical-align:top; border:none; padding-left:20px;">
<p><strong>Decisiones de diseño:</strong> El umbral de 20 píxeles entre muñeca y hombro proporciona robustez ante ruido en la detección de poses de YOLO. La inversión del control (brazo derecho → giro izquierda) resulta más intuitiva para el operador, ya que el robot gira hacia donde &quot;apunta&quot; el brazo elevado desde la perspectiva del usuario.</p>
</td>
</tr>
</table>
<h2 id="2-modos-de-detecci%C3%B3n-y-transici%C3%B3n-a-pol%C3%ADtica-rl">2. MODOS DE DETECCIÓN Y TRANSICIÓN A POLÍTICA RL</h2>
<h3 id="21-detecci%C3%B3n-del-objetivo">2.1 Detección del objetivo</h3>
<p>El sistema soporta dos modos de detección configurables mediante la variable <code>MODE</code>. El modo BLOB (para simulador) utiliza la detección de blobs de Robobo para identificar objetos rojos. Por otro lado, el modo YOLO (para el robot real) emplea YOLOv8n para detectar objetos arbitrarios definidos por la variable <code>TARGET</code> (por defecto &quot;bottle&quot;). El sistema realiza inferencia con el modelo frame a frame hasta detectar el objetivo (umbral de confianza mínimo de 0.5).</p>
<h3 id="22-integraci%C3%B3n-con-pol%C3%ADtica-de-rl">2.2 Integración con política de RL</h3>
<table style="width:100%; border:none;">
<tr>
<td style="width:45%; vertical-align:top; border:none;">
<p><strong>Condiciones de activación:</strong> La política se activa cuando: (1) el objetivo es detectado por la cámara del robot, y (2) el usuario ha completado al menos 5 acciones de teleoperación.</p>
<p><strong>Adaptación de observaciones:</strong> La política espera observaciones discretizadas en 6 sectores. Se adaptan las coordenadas de YOLO normalizando al rango [0, 100] y se mapean a 0-6 (0-1-2-3-4 indican orientación y 5 indica no detectado).</p>
</td>
<td style="width:55%; vertical-align:top; border:none; padding-left:20px;">
<p><strong>Ejecución de la política:</strong></p>
<pre class="hljs"><code><div>action, _ = model.predict(observation, deterministic=<span class="hljs-literal">True</span>)

<span class="hljs-keyword">if</span> action == <span class="hljs-number">0</span>: <span class="hljs-comment"># Avanzar</span>
    rob.moveWheelsByTime(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>)
<span class="hljs-keyword">elif</span> action == <span class="hljs-number">1</span>: <span class="hljs-comment"># Izquierda</span>
    rob.moveWheelsByTime(<span class="hljs-number">10</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>)
<span class="hljs-keyword">elif</span> action == <span class="hljs-number">2</span>: <span class="hljs-comment"># Derecha</span>
    rob.moveWheelsByTime(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.5</span>)
</div></code></pre>
</td>
</tr>
</table>
<p><strong>Condición de éxito:</strong> El episodio finaliza cuando el área del objeto detectado supera 70000 píxeles (modo YOLO) o el blob supera 8000 con posición centrada (modo BLOB).</p>
<h2 id="3-resultados-y-conclusiones">3. RESULTADOS Y CONCLUSIONES</h2>
<h3 id="31-demostraci%C3%B3n-del-sistema">3.1 Demostración del sistema</h3>
<p>El funcionamiento del sistema se demuestra en dos vídeos complementarios. El primero (<code>pov_ordenador.mp4</code>) es en una grabación de pantalla del ordenador que muestra las dos cámaras activas durante la ejecución: la del móvil montado sobre el Robobo, que transmite la perspectiva del robot, y la webcam del ordenador donde el usuario realiza los gestos para dirigir el robot. Este vídeo muestra las anotaciones de gestos de YOLOv8-pose, el objeto detectado por YOLOv8n (si lo hay) las acciones ejecutadas y la activación de la política RL.</p>
<p>El segundo vídeo (<code>pov_persona.mp4</code>) ofrece un punto de vista externo del Robobo durante la ejecución. Se puede observar de nuevo como navega hacia el objetivo: primero bajo el control del usuario y posteriormente de forma autónoma con RL.</p>
<h3 id="32-an%C3%A1lisis-del-sistema">3.2 Análisis del sistema</h3>
<p>Entre las <strong>fortalezas</strong> del sistema destaca la fusión fluida de los componentes de visión artificial (YOLO) y el control gestual, permitiendo una operación intuitiva. La reutilización de la política de RL aprendida en la P.01 demuestra la transferibilidad del aprendizaje entre contextos, mientras que la arquitectura modular con modos BLOB y YOLO intercambiables facilita el desarrollo en simulador y el despliegue en el robot real. El feedback visual proporciona al operador información continua sobre el estado del sistema en tiempo real.</p>
<p>Respecto a las <strong>limitaciones</strong>, la latencia inherente al streaming de vídeo desde el móvil puede introducir retrasos perceptibles en la respuesta del robot ante los comandos. La detección de poses muestra sensibilidad a las condiciones de iluminación del entorno, y el umbral fijo de confianza de 0.5 podría requerir ajustes según las características específicas de cada escenario. Además, la política de RL entrenada exclusivamente en simulador puede manifestar el fenómeno <em>reality gap</em> al ejecutarse en el robot físico.</p>
<h3 id="33-conclusiones">3.3 Conclusiones</h3>
<p>Este trabajo demuestra la viabilidad de sistemas híbridos que combinan teleoperación gestual con control autónomo. La arquitectura modular permite alternar entre control humano y política de refuerzo de forma transparente, aprovechando las fortalezas de cada modo: la intuición humana para navegación de largo alcance y la precisión del agente entrenado para la aproximación final. La integración de YOLOv8 para detección de poses y objetos proporciona una base robusta y extensible para aplicaciones de robótica interactiva en tiempo real.</p>

</body>
</html>
